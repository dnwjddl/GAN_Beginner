- 순환 신경망으로 특정 글의 스타일을 흉내낸 텍스트 seq를 생성하는데 적용
- 문장에서 가능한 질문-대답 쌍 생성

# RNN(처음)
- 순환층은 매우 간단
- tanh() 하나로 구성(time step사이에 정보를 -1~1사이로 scale 맞춤
  - gradient Vanishing(그레이디언트 소실) : **긴 sequence 가진 데이터에 안 좋음**  
***RNN은 격차가 늘수록 학습 정보를 잃어버림***

# LSTM(Long short-term memory)
## 1) 텍스트를 정제하고 token화
✔ ```Tokenize``` : 텍스트를 단어나 문자와 같은 개별 단위로 나누는 작업  

|단어 토큰|문자 토큰|
|:-------------:|:---------:|
|텍스트를 소문자로 변환|대문자는 소문자로 바꾸거나 별도의 토큰으로 담겨두기|
|드물게 나타난거 삭제|문자의 seq를 생성해 훈련 어휘사전에 없는 새로운 단어 제작|
|stemming(단어에서 어간 추출)||
|구두점(.)와(,)를 토큰화 or 제거||
|훈련 어휘사전에 없는 단어는 모델이 예측X|어휘사전은 비교적 매우 작음|
||마지막 출력 층에 학습할 가중치수가 적기 때문에 모델 훈련 속도에 유리
